{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c3b8e0b",
   "metadata": {},
   "source": [
    "<a href='https://www.darshan.ac.in/'> <img src='https://www.darshan.ac.in/Content/media/DU_Logo.svg' width=\"250\" height=\"300\"/></a>\n",
    "<pre>\n",
    "<center><b><h1>Data Mining</b></center>\n",
    "<center><b><h1>Malay Panara | 23010101184 | 22/08/2025</b></center>\n",
    "<pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1422b500",
   "metadata": {},
   "source": [
    "# Apriori Algorithm Implementation Assignment\n",
    "\n",
    "### Objective:\n",
    "You will implement the **Apriori algorithm** from scratch (i.e., without using any libraries like `mlxtend`) to find frequent itemsets and generate association rules.\n",
    "\n",
    "### Dataset:\n",
    "Use the [Online Retail Dataset](https://www.kaggle.com/datasets/vijayuv/onlineretail) from Kaggle. You can filter it for a specific country (e.g., `United Kingdom`) and time range to reduce size if needed.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85128a0",
   "metadata": {},
   "source": [
    "## Step 1: Data Preprocessing\n",
    "\n",
    "- Load the dataset\n",
    "- Remove rows with missing values\n",
    "- Filter out rows where `Quantity <= 0`\n",
    "- Convert Data into Basket Format\n",
    "\n",
    "ðŸ‘‰ **Implement code below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc033114",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd66a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Preprocess as per the instructions above | We have already done in TASK 2\n",
    "\n",
    "\n",
    "# Your Code Here\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(\"OnlineRetail.csv\", encoding='unicode_escape')\n",
    "\n",
    "# Step 2: Remove rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 3: Keep only rows with Quantity > 0\n",
    "df = df[df['Quantity'] > 0]\n",
    "\n",
    "# for Basket\n",
    "basket = df.groupby(['InvoiceNo', 'Description'])['Quantity'].sum().unstack().reset_index().fillna(0).set_index('InvoiceNo')\n",
    "basket = basket.applymap(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a37baf6",
   "metadata": {},
   "source": [
    "## Step 2: Implement Apriori Algorithm\n",
    "Step-by-Step Procedure:\n",
    "1. Generate Frequent 1-Itemsets\n",
    "Count the frequency (support) of each individual item in the dataset.\n",
    "Keep only those with support â‰¥ min_support.\n",
    "â†’ Result is L1 (frequent 1-itemsets)\n",
    "2. Iterative Candidate Generation (k = 2 to n)\n",
    "While L(k-1) is not empty:\n",
    "a. Candidate Generation\n",
    "\n",
    "Generate candidate itemsets Ck of size k from L(k-1) using the Apriori property:\n",
    "Any (k-itemset) is only frequent if all of its (kâˆ’1)-subsets are frequent.\n",
    "b. Prune Candidates\n",
    "Eliminate candidates that have any (kâˆ’1)-subset not in L(k-1).\n",
    "c. Count Support\n",
    "For each transaction, count how many times each candidate in Ck appears.\n",
    "d. Generate Frequent Itemsets\n",
    "Form Lk by keeping candidates from Ck that meet the min_support.\n",
    "Repeat until Lk becomes empty.\n",
    "Implement the following functions:\n",
    "1. `get_frequent_itemsets(transactions, min_support)` - Returns frequent itemsets and their support\n",
    "2. `generate_candidates(prev_frequent_itemsets, k)` - Generates candidate itemsets of length `k`\n",
    "3. `calculate_support(transactions, candidates)` - Calculates the support count for each candidate\n",
    "\n",
    "**Write reusable functions** for each part of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8f9310a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement apriori functions below\n",
    "def generate_candidates(prev_frequent_itemsets, k):\n",
    "    \"\"\"\n",
    "    Generate candidate itemsets of size k from frequent itemsets of size k-1\n",
    "    \"\"\"\n",
    "    \n",
    "    candidates = []\n",
    "    prev_list = list(prev_frequent_itemsets)\n",
    "    \n",
    "    for i in range(len(prev_list)):\n",
    "        for j in range(i+1,len(prev_list)):\n",
    "            union_set = prev_list[i].union(prev_list[j])\n",
    "            if len(union_set) == k and union_set not in candidates:\n",
    "                candidates.append(union_set)\n",
    "    \n",
    "    return candidates\n",
    "\n",
    "def calculate_support(transactions, candidates):\n",
    "    \"\"\"\n",
    "    Calculate support for each candidate in transactions\n",
    "    Returns dictionary {itemset: support}\n",
    "    \"\"\"\n",
    "    \n",
    "    support_counts = {c:0 for c in candidates}\n",
    "    num_transactions = len(transactions)\n",
    "    \n",
    "    for t in transactions:\n",
    "        for c in candidates:\n",
    "            if c.issubset(t):\n",
    "                support_counts[c] += 1\n",
    "    \n",
    "    #Convert to support values\n",
    "    support_data = {c : support_counts[c]/num_transactions for c in support_counts}\n",
    "    return support_data\n",
    "\n",
    "def get_frequent_itemsets(transactions, min_support):\n",
    "    \"\"\"\n",
    "    Main Apriori frequent itemset generator\n",
    "    \"\"\"\n",
    "    transactions = list(map(set,transactions))\n",
    "    single_items = set(item for t in transactions for item in t)\n",
    "\n",
    "    # First level: 1-itemsets\n",
    "    candidates = [frozenset([i]) for i in single_items]\n",
    "    support_data = calculate_support(transactions,candidates)\n",
    "    frequent_itemsets = {frozenset(i) : s for i,s in support_data.items() if s >= min_support}\n",
    "    \n",
    "    all_frequent_itemsets = dict(frequent_itemsets)\n",
    "    k = 2\n",
    "    \n",
    "    while frequent_itemsets:\n",
    "        candidates = generate_candidates(frequent_itemsets.keys(),k)\n",
    "        support_data = calculate_support(transactions,candidates)\n",
    "        frequent_itemsets = {frozenset(i) : s for i,s in support_data.items() if s >= min_support}\n",
    "        all_frequent_itemsets.update(frequent_itemsets)\n",
    "        k += 1\n",
    "    \n",
    "    return all_frequent_itemsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb8c0fe",
   "metadata": {},
   "source": [
    "## Step 3: Generate Association Rules\n",
    "\n",
    "- Use frequent itemsets to generate association rules\n",
    "- For each rule `A => B`, calculate:\n",
    "  - **Support**\n",
    "  - **Confidence**\n",
    "- Only return rules that meet a minimum confidence threshold (e.g., 0.5)\n",
    "\n",
    "ðŸ‘‰ **Implement rule generation function below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97bc236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate rules from frequent itemsets\n",
    "\n",
    "def generate_rules(frequent_itemsets, min_confidence):\n",
    "    \"\"\"\n",
    "    Generate association rules from frequent itemsets\n",
    "    \"\"\"\n",
    "    rules = []\n",
    "    \n",
    "    for itemset,support in frequent_itemsets.items():\n",
    "        if len(itemset) >= 2:\n",
    "            for i in range(1,len(itemset)):\n",
    "                for antecedent in combinations(itemset,i):\n",
    "                    antecedent = frozenset(antecedent)\n",
    "                    consequent = itemset - antecedent\n",
    "                    if antecedent in frequent_itemsets:\n",
    "                        confidence = support/frequent_itemsets[antecedent]\n",
    "                        if confidence >= min_confidence:\n",
    "                            rules.append((antecedent,consequent,support,confidence))\n",
    "\n",
    "    return rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf26889",
   "metadata": {},
   "source": [
    "## Step 4: Output and Visualize\n",
    "\n",
    "- Print top 10 frequent itemsets\n",
    "- Print top 10 association rules (by confidence or lift)\n",
    "\n",
    "ðŸ‘‰ **Output results below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3443a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Frequent Itemsets:\n",
      "{'WHITE HANGING HEART T-LIGHT HOLDER'}: support = 0.11\n",
      "{'REGENCY CAKESTAND 3 TIER'}: support = 0.09\n",
      "{'JUMBO BAG RED RETROSPOT'}: support = 0.09\n",
      "{'PARTY BUNTING'}: support = 0.07\n",
      "{'ASSORTED COLOUR BIRD ORNAMENT'}: support = 0.07\n",
      "{'LUNCH BAG RED RETROSPOT'}: support = 0.07\n",
      "{'SET OF 3 CAKE TINS PANTRY DESIGN '}: support = 0.06\n",
      "{'POSTAGE'}: support = 0.06\n",
      "{'LUNCH BAG  BLACK SKULL.'}: support = 0.06\n",
      "{'PACK OF 72 RETROSPOT CAKE CASES'}: support = 0.06\n",
      "\n",
      "Top 10 Association Rules:\n",
      "{'PINK REGENCY TEACUP AND SAUCER', 'ROSES REGENCY TEACUP AND SAUCER '} -> {'GREEN REGENCY TEACUP AND SAUCER'}, support = 0.02, confidence = 0.89\n",
      "{'PINK REGENCY TEACUP AND SAUCER', 'GREEN REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER '}, support = 0.02, confidence = 0.85\n",
      "{'PINK REGENCY TEACUP AND SAUCER'} -> {'GREEN REGENCY TEACUP AND SAUCER'}, support = 0.02, confidence = 0.83\n",
      "{'PINK REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER '}, support = 0.02, confidence = 0.78\n",
      "{'GREEN REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER '}, support = 0.03, confidence = 0.78\n",
      "{'GARDENERS KNEELING PAD CUP OF TEA '} -> {'GARDENERS KNEELING PAD KEEP CALM '}, support = 0.02, confidence = 0.73\n",
      "{'ROSES REGENCY TEACUP AND SAUCER ', 'GREEN REGENCY TEACUP AND SAUCER'} -> {'PINK REGENCY TEACUP AND SAUCER'}, support = 0.02, confidence = 0.72\n",
      "{'PINK REGENCY TEACUP AND SAUCER'} -> {'ROSES REGENCY TEACUP AND SAUCER ', 'GREEN REGENCY TEACUP AND SAUCER'}, support = 0.02, confidence = 0.70\n",
      "{'ROSES REGENCY TEACUP AND SAUCER '} -> {'GREEN REGENCY TEACUP AND SAUCER'}, support = 0.03, confidence = 0.69\n",
      "{'DOLLY GIRL LUNCH BOX'} -> {'SPACEBOY LUNCH BOX '}, support = 0.02, confidence = 0.69\n"
     ]
    }
   ],
   "source": [
    "# Output the final results\n",
    "# Optional: Add visualizations\n",
    "\n",
    "# Parameters\n",
    "min_support = 0.02\n",
    "min_confidence = 0.05\n",
    "\n",
    "# Generate frequent itemsets and rules\n",
    "transactions = basket.apply(lambda row: set(row[row==1].index), axis=1).tolist()\n",
    "frequent_itemsets = get_frequent_itemsets(transactions, min_support)\n",
    "rules = generate_rules(frequent_itemsets, min_confidence)\n",
    "\n",
    "# Top 10 frequent itemsets\n",
    "top_freq = sorted(frequent_itemsets.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "print(\"Top 10 Frequent Itemsets:\")\n",
    "for itemset, support in top_freq:\n",
    "    print(f\"{set(itemset)}: support = {support:.2f}\")\n",
    "\n",
    "# Top 10 association rules\n",
    "top_rules = sorted(rules, key=lambda x: x[3], reverse=True)[:10]\n",
    "print(\"\\nTop 10 Association Rules:\")\n",
    "for antecedent, consequent, support, confidence in top_rules:\n",
    "    print(f\"{set(antecedent)} -> {set(consequent)}, support = {support:.2f}, confidence = {confidence:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35083bb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
